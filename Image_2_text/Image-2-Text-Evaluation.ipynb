{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ultralytics","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:00:02.741425Z","iopub.execute_input":"2024-07-24T13:00:02.741842Z","iopub.status.idle":"2024-07-24T13:00:16.519892Z","shell.execute_reply.started":"2024-07-24T13:00:02.741801Z","shell.execute_reply":"2024-07-24T13:00:16.518720Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: ultralytics in /opt/conda/lib/python3.10/site-packages (8.2.64)\nRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.10.0.84)\nRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.5.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (6.0.1)\nRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.11.4)\nRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.1.2)\nRequirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.16.2)\nRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.66.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.2.2)\nRequirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.12.2)\nRequirement already satisfied: ultralytics-thop>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.0.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2024.5.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport numpy as np\nimport torch\nfrom transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom ultralytics import YOLO\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:29:35.059652Z","iopub.execute_input":"2024-07-24T15:29:35.060493Z","iopub.status.idle":"2024-07-24T15:29:35.065706Z","shell.execute_reply.started":"2024-07-24T15:29:35.060462Z","shell.execute_reply":"2024-07-24T15:29:35.064729Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Überprüfen, ob eine GPU verfügbar ist\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:15:53.598594Z","iopub.execute_input":"2024-07-24T13:15:53.599280Z","iopub.status.idle":"2024-07-24T13:15:53.603490Z","shell.execute_reply.started":"2024-07-24T13:15:53.599240Z","shell.execute_reply":"2024-07-24T13:15:53.602561Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Laden der Modelle","metadata":{}},{"cell_type":"code","source":"# Laden des vortrainierten Modells und der entsprechenden Tokenizer\nmodel = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\nfeature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\ntokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n\n# Laden des BLIP Modell und der entsprechenden Tokenizer\nblip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nblip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n#Landen YOLO\nmodel_yolo = YOLO('yolov8n.pt')  ","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:15:56.188025Z","iopub.execute_input":"2024-07-24T13:15:56.188649Z","iopub.status.idle":"2024-07-24T13:16:04.192531Z","shell.execute_reply.started":"2024-07-24T13:15:56.188618Z","shell.execute_reply":"2024-07-24T13:16:04.191562Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Modelle laden und auf mehrere GPUs verteilen\nmodel = torch.nn.DataParallel(model).to(device)\nblip_model = torch.nn.DataParallel(blip_model).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:32:46.883928Z","iopub.execute_input":"2024-07-24T15:32:46.884633Z","iopub.status.idle":"2024-07-24T15:32:46.907201Z","shell.execute_reply.started":"2024-07-24T15:32:46.884601Z","shell.execute_reply":"2024-07-24T15:32:46.906441Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# Caption Generation","metadata":{}},{"cell_type":"code","source":"# Funktion zum Generieren der Bildbeschreibung mit ViT-GPT2\ndef generate_caption(image_path):\n    image = Image.open(image_path).convert(\"RGB\")\n    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(device)  # Auf GPU verschieben\n    attention_mask = torch.ones(pixel_values.shape[:2], dtype=torch.long).to(device)  # Auf GPU verschieben\n    output_ids = model.module.generate(pixel_values, attention_mask=attention_mask, max_length=16, num_beams=4)\n    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    return caption\n\n# Funktion zum Generieren der Bildbeschreibung mit BLIP\ndef generate_blip_caption(image_path):\n    image = Image.open(image_path).convert(\"RGB\") \n    inputs = blip_processor(images=image, return_tensors=\"pt\").to(device)  # Auf GPU verschieben\n\n    output_ids = blip_model.module.generate(**inputs, max_length=16, num_beams=4)\n    caption = blip_processor.decode(output_ids[0], skip_special_tokens=True)\n    return caption\n\n# Funktion zur Objekterkennung und Generierung einfacher Bildunterschriften\ndef detect_objects(image_path):\n    # Laden des Bildes\n    image = Image.open(image_path).convert(\"RGB\")\n    image_np = np.array(image)\n\n    # Anwenden des YOLOv8 auf das Bild\n    results = model_yolo(image_np, verbose=False)  # Deaktiviert die Anzeige des Bildes\n\n    # Extrahieren der erkannten Objekte\n    detected_objects = []\n    for result in results:\n        for box in result.boxes:\n            cls_id = int(box.cls[0])\n            label = model_yolo.names[cls_id]\n            detected_objects.append(label)\n\n    return detected_objects\n\ndef generate_simple_caption(detected_objects):\n    if not detected_objects:\n        return \"No objects detected in the image.\"\n    \n    unique_objects = set(detected_objects)\n    object_counts = {obj: detected_objects.count(obj) for obj in unique_objects}\n    \n    caption = \"The image contains \"\n    for obj, count in object_counts.items():\n        if count == 1:\n            caption += f\"a {obj}, \"\n        else:\n            caption += f\"{count} {obj}s, \"\n    \n    caption = caption.rstrip(', ') + '.'\n    return caption\n","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:18:34.622003Z","iopub.execute_input":"2024-07-24T16:18:34.622403Z","iopub.status.idle":"2024-07-24T16:18:34.634725Z","shell.execute_reply.started":"2024-07-24T16:18:34.622372Z","shell.execute_reply":"2024-07-24T16:18:34.633782Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"# Load und Evaluation Funktionen","metadata":{}},{"cell_type":"code","source":"# Funktion zum Laden der Bildunterschriften aus der Textdatei\ndef load_captions(caption_file):\n    with open(caption_file, 'r') as file:\n        lines = file.readlines()\n    caption_dict = {}\n    for line in lines[1:]:  # Überspringen der Header-Zeile\n        parts = line.strip().split(',')\n        image_id = parts[0]\n        caption = ','.join(parts[1:])\n        if image_id not in caption_dict:\n            caption_dict[image_id] = []\n        caption_dict[image_id].append(caption)\n    return caption_dict\n\n# Funktion zur Evaluierung der generierten Bildunterschriften mit detaillierter Analyse\ndef evaluate_model_detailed(caption_dict, image_dir, model_func, num_images=8000, smoothing_method=None):\n    scores = []\n    n_gram_scores = {1: [], 2: [], 3: [], 4: []}\n    \n    limited_keys = list(caption_dict.keys())[:num_images]\n    \n    if smoothing_method:\n        smoothing_function = getattr(SmoothingFunction(), smoothing_method)\n    else:\n        smoothing_function = None\n    \n    for image_id in limited_keys:\n        reference_captions = caption_dict[image_id]\n        image_path = os.path.join(image_dir, image_id)\n        generated_caption = model_func(image_path)\n        \n        # Berechnen der n-Gram Präzisionen\n        precisions = []\n        for n in range(1, 5):\n            weights = [1.0 / n] * n + [0] * (4 - n)\n            n_gram_score = sentence_bleu([ref.split() for ref in reference_captions], generated_caption.split(), weights=weights[:n], smoothing_function=smoothing_function)\n            n_gram_scores[n].append(n_gram_score)\n            precisions.append(n_gram_score)\n        \n        # Hinzufügen einer kleinen Konstante, um log(0) zu vermeiden\n        precisions = [p if p > 0 else 1e-9 for p in precisions]\n        \n        # Berechnen des geometrischen Mittels der n-Gram Präzisionen\n        geometric_mean = np.exp(np.mean(np.log(precisions)))\n        \n        # Brevity Penalty\n        ref_lengths = [len(ref.split()) for ref in reference_captions]\n        hyp_length = len(generated_caption.split())\n        closest_ref_length = min(ref_lengths, key=lambda ref_len: (abs(ref_len - hyp_length), ref_len))\n        if hyp_length > closest_ref_length:\n            brevity_penalty = 1\n        else:\n            brevity_penalty = np.exp(1 - closest_ref_length / hyp_length)\n        \n        # BLEU Score\n        bleu_score = brevity_penalty * geometric_mean\n        scores.append(bleu_score)\n    \n    average_bleu = np.mean(scores)\n    average_n_gram_scores = {n: np.mean(scores) for n, scores in n_gram_scores.items()}\n    \n    return average_bleu, average_n_gram_scores","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:16:04.804012Z","iopub.execute_input":"2024-07-24T13:16:04.804333Z","iopub.status.idle":"2024-07-24T13:16:04.823796Z","shell.execute_reply.started":"2024-07-24T13:16:04.804302Z","shell.execute_reply":"2024-07-24T13:16:04.822802Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Pfad zum Flickr8k-Bilderverzeichnis und zur Bildunterschriften-Datei\nimage_dir = '/kaggle/input/flickr8k/Images'\ncaption_file = '/kaggle/input/flickr8k/captions.txt'\n    \n# Laden der Bildunterschriften\ncaption_dict = load_captions(caption_file)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:16:08.169055Z","iopub.execute_input":"2024-07-24T13:16:08.169918Z","iopub.status.idle":"2024-07-24T13:16:08.239714Z","shell.execute_reply.started":"2024-07-24T13:16:08.169884Z","shell.execute_reply":"2024-07-24T13:16:08.238726Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Evaluierungsergebnisse","metadata":{}},{"cell_type":"code","source":"# Berechnung der BLEU-Scores für das ViT-GPT2\naverage_bleu_method4, n_gram_scores_method4 = evaluate_model_detailed(caption_dict, image_dir, generate_caption, num_images=8000, smoothing_method='method4')\n\n# Ausgabe der BLEU-Scores und n-Gramm-Präzisionen\nprint(f\"Average BLEU (method4): {average_bleu_method4}\")\nprint(f\"1-Gram Precision (method4): {n_gram_scores_method4[1]}\")\nprint(f\"2-Gram Precision (method4): {n_gram_scores_method4[2]}\")\nprint(f\"3-Gram Precision (method4): {n_gram_scores_method4[3]}\")\nprint(f\"4-Gram Precision (method4): {n_gram_scores_method4[4]}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:16:16.388065Z","iopub.execute_input":"2024-07-24T13:16:16.388449Z","iopub.status.idle":"2024-07-24T14:15:15.755728Z","shell.execute_reply.started":"2024-07-24T13:16:16.388418Z","shell.execute_reply":"2024-07-24T14:15:15.754727Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Average BLEU (method4): 0.3493923512893618\n1-Gram Precision (method4): 0.5522633022052464\n2-Gram Precision (method4): 0.3836246752095355\n3-Gram Precision (method4): 0.3136509277948999\n4-Gram Precision (method4): 0.27369742485294796\n","output_type":"stream"}]},{"cell_type":"code","source":"# Berechnung der BLEU-Scores für das BLIP\naverage_bleu_method4, n_gram_scores_method4 = evaluate_model_detailed(caption_dict, image_dir, generate_blip_caption, num_images=8000, smoothing_method='method4')\n\n# Ausgabe der BLEU-Scores und n-Gramm-Präzisionen\nprint(f\"Average BLEU (method4) for BLIP: {average_bleu_method4}\")\nprint(f\"1-Gram Precision (method4): {n_gram_scores_method4[1]}\")\nprint(f\"2-Gram Precision (method4): {n_gram_scores_method4[2]}\")\nprint(f\"3-Gram Precision (method4): {n_gram_scores_method4[3]}\")\nprint(f\"4-Gram Precision (method4): {n_gram_scores_method4[4]}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-24T14:18:46.392635Z","iopub.execute_input":"2024-07-24T14:18:46.393477Z","iopub.status.idle":"2024-07-24T15:08:33.847830Z","shell.execute_reply.started":"2024-07-24T14:18:46.393438Z","shell.execute_reply":"2024-07-24T15:08:33.846895Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Average BLEU (method4) for BLIP: 0.32479506854341345\n1-Gram Precision (method4): 0.5623407034727911\n2-Gram Precision (method4): 0.4147418525669168\n3-Gram Precision (method4): 0.3321488673375305\n4-Gram Precision (method4): 0.2786913487389565\n","output_type":"stream"}]},{"cell_type":"code","source":"# Berechnung der BLEU-Scores für das YOLO\naverage_bleu_method4, n_gram_scores_method4 = evaluate_model_detailed(caption_dict, image_dir, lambda img_path: generate_simple_caption(detect_objects(img_path)), num_images=8000, smoothing_method='method4')\n# Ausgabe der BLEU-Scores und n-Gramm-Präzisionen\nprint(f\"Average BLEU (method4) for YOLO: {average_bleu_method4}\")\nprint(f\"1-Gram Precision (method4): {n_gram_scores_method4[1]}\")\nprint(f\"2-Gram Precision (method4): {n_gram_scores_method4[2]}\")\nprint(f\"3-Gram Precision (method4): {n_gram_scores_method4[3]}\")\nprint(f\"4-Gram Precision (method4): {n_gram_scores_method4[4]}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:18:38.937896Z","iopub.execute_input":"2024-07-24T16:18:38.938263Z","iopub.status.idle":"2024-07-24T16:20:56.690330Z","shell.execute_reply.started":"2024-07-24T16:18:38.938234Z","shell.execute_reply":"2024-07-24T16:20:56.689378Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Average BLEU (method4) for YOLO: 0.11739814950202587\n1-Gram Precision (method4): 0.1419689757950447\n2-Gram Precision (method4): 0.1503721345494763\n3-Gram Precision (method4): 0.1432906352992302\n4-Gram Precision (method4): 0.13340715040035322\n","output_type":"stream"}]}]}